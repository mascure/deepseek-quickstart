{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/deepseek/bin/python\n",
      "Python 3.13.7\n",
      "Name: torch\n",
      "Version: 2.7.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /root/miniconda3/envs/deepseek/lib/python3.13/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, setuptools, sympy, triton, typing-extensions\n",
      "Required-by: accelerate, bitsandbytes, cut-cross-entropy, peft, timm, torchvision, unsloth, unsloth_zoo, xformers\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!python --version\n",
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/deepseek/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.5: Fast Qwen2 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.581 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰æ¨¡å‹å’Œä¸€äº›åŸºæœ¬å‚æ•°\n",
    "max_seq_length = 8192\n",
    "dtype = None # None è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹© (Float16 a T4, V100, BFloat16 a Ampere)\n",
    "load_in_4bit = True # ä½¿ç”¨ 4bit é‡åŒ–åŠ è½½\n",
    "\n",
    "# è¿™æ˜¯æ‚¨çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œè¯·æ›¿æ¢ä¸ºæ‚¨æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹\n",
    "# ä¾‹å¦‚ï¼š\"qwen-1.5b_lora_model\"\n",
    "# model_name = \"qwen-1.5b_lora_model\" \n",
    "# model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\" \n",
    "model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\" \n",
    "\n",
    "# è¿™ä¸€æ­¥ä¼šè¿”å›ä¸€ä¸ªç»è¿‡ Unsloth ä¼˜åŒ–çš„æ¨¡å‹å’Œä¸€ä¸ªåˆ†è¯å™¨\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = model_name,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "# )\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./DeepSeek-R1-Qwen-1.5B-4bit\",  # æœ¬åœ°ç›®å½•\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    local_files_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_dir = \"qwen-1.5b_lora_model\"    # ä¿å­˜çš„ lora ç›®å½•\n",
    "base_model=model\n",
    "# ä½¿ç”¨ PEFT å°† adapter åŠ è½½åˆ° base_model ä¸Š\n",
    "model = PeftModel.from_pretrained(model, adapter_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. å¾®è°ƒå‰æ¨ç†æµ‹è¯•\n",
    "\n",
    "åœ¨å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆç”¨å®ƒæ¥ç”Ÿæˆä¸€æ®µæ–‡æœ¬ï¼Œçœ‹çœ‹åŸå§‹æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ã€‚è¿™å¯ä»¥ä½œä¸ºæˆ‘ä»¬å¾®è°ƒæ•ˆæœçš„åŸºå‡†å‚è€ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹æ¨ç†çš„ Prompt æ¨¡æ¿\n",
    "inference_prompt = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
    "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "question = \"ç”·ï¼Œ28å²ï¼Œç¨‹åºå‘˜ï¼Œæœ€è¿‘ä¸€å‘¨æ¯å¤©å·¥ä½œåˆ°åŠå¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œæœ‰æ—¶å€™è¿˜æ¶å¿ƒã€‚\"\n",
    "\n",
    "inputs = tokenizer([inference_prompt.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "attention_mask = inputs.input_ids.ne(tokenizer.pad_token_id).long().to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "é¦–å…ˆï¼Œä½ æåˆ°ä»–æ˜¯ä¸€å28å²çš„ç¨‹åºå‘˜ï¼Œå·¥ä½œåˆ°æ·±å¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œè¿˜ sometimesæ¶å¿ƒã€‚è¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°ä»–å¯èƒ½æœ‰è„‘æŸä¼¤ï¼Œç‰¹åˆ«æ˜¯è„‘éƒ¨çš„æŸä¼¤å¯èƒ½æ¯”è¾ƒå¸¸è§ã€‚å¸¸è§çš„è„‘æŸä¼¤æœ‰è„‘è†œç‚ã€è„‘è†œç‚æ€§è„‘ç‚ã€è„‘è†œç‚æ€§è„‘è†œç‚ä»¥åŠè„‘è†œç‚æ€§è„‘è†œç‚ç­‰ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œç»“åˆä»–æœ€è¿‘çš„å·¥ä½œè¡¨ç°ï¼Œå·¥ä½œåˆ°æ·±å¤œå¾ˆç–²åŠ³ï¼Œè¯´æ˜å¯èƒ½æœ‰è„‘éƒ¨çš„æŸä¼¤å¯¼è‡´çš„ç–²åŠ³ã€‚è„‘è†œç‚é€šå¸¸ä¼šå¯¼è‡´è¿™ç§ç—‡çŠ¶ï¼Œè€Œè„‘è†œç‚æ€§è„‘è†œç‚åˆ™å¯èƒ½æœ‰æ›´ä¸¥é‡çš„ç—‡çŠ¶ã€‚\n",
      "\n",
      "ç„¶åï¼Œè€ƒè™‘ä»–æœ€è¿‘å·¥ä½œåˆ°æ·±å¤œçš„æƒ…å†µï¼Œè¿™å¯èƒ½ä¸è„‘è†œç‚æœ‰å…³ã€‚è„‘è†œç‚å¯ä»¥å¯¼è‡´ç–²åŠ³ã€æ¶å¿ƒã€å¤´ç—›å’Œå¤´æ™•ï¼Œè¿™äº›ç—‡çŠ¶æ­£æ˜¯ä½ æåˆ°çš„ã€‚\n",
      "\n",
      "å†æƒ³æƒ³è„‘è†œç‚çš„ç±»å‹ï¼Œè„‘è†œç‚åˆ†ä¸ºæ€¥æ€§å’Œæ…¢æ€§ä¸¤ç§ï¼Œæ€¥æ€§è„‘è†œç‚å¯èƒ½æœ‰æ›´ä¸¥é‡çš„ç—‡çŠ¶ï¼Œè€Œæ…¢æ€§è„‘è†œç‚åˆ™å¯èƒ½æ›´ç¨³å®šã€‚è¿™å–å†³äºä»–çš„å…·ä½“ç—…æƒ…å’Œæ²»ç–—æƒ…å†µã€‚\n",
      "\n",
      "æ­¤å¤–ï¼Œè„‘è†œç‚æ€§è„‘è†œç‚æ˜¯æ€¥æ€§è„‘è†œç‚çš„ä¸€ç§ï¼Œå®ƒé€šå¸¸ä¸æ€¥æ€§è„‘è†œç‚æœ‰å…³ï¼Œå¯èƒ½ä¼šæœ‰æ›´ä¸¥é‡çš„ç—‡çŠ¶ã€‚æ‰€ä»¥ç»“åˆä»–çš„ç—‡çŠ¶å’Œå·¥ä½œè¡¨ç°ï¼Œè„‘è†œç‚æ€§è„‘è†œç‚å¯èƒ½æ›´ç¬¦åˆä»–çš„æè¿°ã€‚\n",
      "\n",
      "è¿˜æœ‰å…¶ä»–è„‘è†œç‚çš„ç±»å‹ï¼Œæ¯”å¦‚æ€¥æ€§è„‘è†œç‚å’Œæ€¥æ€§è„‘è†œç‚æ€§è„‘è†œç‚ï¼Œè¿™ä¸¤ç§éƒ½å¯èƒ½å¯¼è‡´å’Œä½ æåˆ°çš„ç±»ä¼¼ç—‡çŠ¶ã€‚æ€¥æ€§è„‘è†œç‚é€šå¸¸ä¼šå¯¼è‡´æ›´ä¸¥é‡çš„ç—‡çŠ¶ï¼Œè€Œæ€¥æ€§è„‘è†œç‚æ€§è„‘è†œç‚å¯èƒ½æ›´ç¨³å®šã€‚\n",
      "\n",
      "æ‰€ä»¥ç»¼åˆæ¥çœ‹ï¼Œè„‘è†œç‚å’Œè„‘è†œç‚æ€§è„‘è†œç‚éƒ½æ˜¯æ¯”è¾ƒç¬¦åˆä½ æåˆ°çš„ç—‡çŠ¶çš„ã€‚å› æ­¤ï¼Œç»“åˆè¿™äº›ä¿¡æ¯ï¼Œæˆ‘è®¤ä¸ºè„‘è†œç‚æ€§è„‘è†œç‚å¯èƒ½æ˜¯ä½ æåˆ°çš„è¿™ç§æƒ…å†µï¼Œå› æ­¤é€‰æ‹©è¿™ä¸ªé€‰é¡¹æ¯”è¾ƒåˆé€‚ã€‚\n",
      "</think>\n",
      "æ ¹æ®ä½ æè¿°çš„ç—‡çŠ¶å’Œç—‡çŠ¶ï¼Œç”·28å²ç¨‹åºå‘˜æœ€è¿‘å·¥ä½œåˆ°æ·±å¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œè¿˜å¶å°”æ¶å¿ƒï¼Œè¿™äº›ç—‡çŠ¶æç¤ºå¯èƒ½æœ‰è„‘éƒ¨æŸä¼¤ã€‚é€šå¸¸ï¼Œè„‘è†œç‚å’Œè„‘è†œç‚æ€§è„‘è†œç‚æ˜¯å¸¸è§çš„è„‘éƒ¨æŸä¼¤ç±»å‹ã€‚è„‘è†œç‚å¯èƒ½å¯¼è‡´è¿™äº›ç—‡çŠ¶ï¼Œè€Œè„‘è†œç‚æ€§è„‘è†œç‚åˆ™å¯èƒ½æœ‰æ›´ä¸¥é‡çš„ç—‡çŠ¶ã€‚ç»“åˆç—‡çŠ¶å’Œå·¥ä½œè¡¨ç°ï¼Œè„‘è†œç‚æ€§è„‘è†œç‚å¯èƒ½æ˜¯ä½ æåˆ°çš„è¿™ç§æƒ…å†µï¼Œå› æ­¤é€‰æ‹©è¿™ä¸ªé€‰é¡¹æ¯”è¾ƒåˆé€‚ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. æµ‹è¯•è®­ç»ƒåçš„ç”Ÿæˆç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "question=\"ä¸€ä¸ªæ‚£æœ‰æ€¥æ€§é˜‘å°¾ç‚çš„ç—…äººå·²ç»å‘ç—…5å¤©ï¼Œè…¹ç—›ç¨æœ‰å‡è½»ä½†ä»ç„¶å‘çƒ­ï¼Œåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ï¼Œæ­¤æ—¶åº”å¦‚ä½•å¤„ç†ï¼Ÿ\", # Question\n",
    "inputs = tokenizer([inference_prompt.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1000,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "é¦–å…ˆï¼Œç—…äººå·²ç»è¯Šæ–­ä¸ºæ€¥æ€§é˜‘å°¾ç‚ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´è…¹ç—›ã€å‘çƒ­ã€å‹ç—›åŒ…å—ç­‰ç—‡çŠ¶ã€‚5å¤©åï¼Œç—…äººå‡ºç°è½»åº¦çš„è…¹ç—›å’Œå‘çƒ­ï¼Œä½†å‹ç—›åŒ…å—ä»ç„¶å­˜åœ¨ã€‚\n",
      "\n",
      "æ ¹æ®ä¸´åºŠå®è·µï¼Œç—…äººå‡ºç°å‹ç—›åŒ…å—å¯èƒ½éœ€è¦ç«‹å³å¤„ç†ï¼Œä»¥é˜²æ­¢æ„ŸæŸ“æ‰©æ•£å’Œé™ä½æ„ŸæŸ“é£é™©ã€‚\n",
      "\n",
      "åœ¨æ€¥æ€§é˜‘å°¾ç‚çš„æ—©æœŸï¼Œå¯ä»¥å°è¯•é€šè¿‡è¯ç‰©æ²»ç–—æ¥ç¼“è§£ç—‡çŠ¶ã€‚æ¯”å¦‚ï¼Œä½¿ç”¨æŠ—ç‚è¯ç‰©ï¼Œå¦‚é˜¿å¸åŒ¹æ—ï¼Œä»¥å‡å°‘ç‚ç—‡ååº”ã€‚\n",
      "\n",
      "åŒæ—¶ï¼Œè€ƒè™‘è¿›è¡Œæ— åˆ›æ²»ç–—ï¼Œå¦‚æ‰‹æœ¯æˆ–è¯ç‰©æ²»ç–—ï¼Œä»¥å¸®åŠ©å‡è½»ç—‡çŠ¶å¹¶é˜²æ­¢æ„ŸæŸ“ã€‚\n",
      "\n",
      "è¿™äº›æªæ–½å¯ä»¥å¸®åŠ©ç—…äººå‡è½»ç—‡çŠ¶ï¼Œä½†éœ€è¦æ ¹æ®å…·ä½“ç—…å²å’Œç—…äººçš„ååº”æ¥è°ƒæ•´æ²»ç–—æ–¹æ¡ˆã€‚\n",
      "\n",
      "åœ¨å¤„ç†å‹ç—›åŒ…å—ä¹‹å‰ï¼Œç—…äººæœ€å¥½å°½å¿«è¿›è¡Œä¸€äº›è¯ç‰©æ²»ç–—ï¼Œä»¥é˜²æ­¢æ„ŸæŸ“æ‰©æ•£å’Œé™ä½æ„ŸæŸ“é£é™©ã€‚\n",
      "\n",
      "å¯¹äºæ€¥æ€§é˜‘å°¾ç‚æ‚£è€…ï¼ŒåŠæ—¶å¤„ç†å‹ç—›åŒ…å—å¯ä»¥æœ‰æ•ˆå‡å°‘æ„ŸæŸ“çš„å‘ç”Ÿï¼ŒåŒæ—¶ä¹Ÿèƒ½å‡è½»æ‚£è€…çš„ç—‡çŠ¶å’Œå¹¶å‘ç—‡çš„é£é™©ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œé’ˆå¯¹ç—…äººå½“å‰çš„ç—‡çŠ¶ï¼Œåº”ç«‹å³å¼€å§‹è¯ç‰©æ²»ç–—å’Œæ— åˆ›æ²»ç–—ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œæ›´æ·±å…¥çš„è¯„ä¼°å’Œæ²»ç–—ã€‚\n",
      "</think>\n",
      "æ ¹æ®ç—…äººçš„æƒ…å†µï¼Œæ€¥æ€§é˜‘å°¾ç‚çš„æ—©æœŸç—‡çŠ¶åŒ…æ‹¬è…¹ç—›ã€å‘çƒ­å’Œå‹ç—›åŒ…å—ã€‚ç”±äºç—…äººå·²ç»å‘ç—…5å¤©ï¼Œç—‡çŠ¶å·²ç»è½»å¾®å‡è½»ï¼Œä½†å‹ç—›åŒ…å—ä»ç„¶å­˜åœ¨ï¼Œè¿™éœ€è¦ç«‹å³é‡‡å–æªæ–½ä»¥é˜²æ­¢æ„ŸæŸ“æ‰©æ•£ã€‚\n",
      "\n",
      "åœ¨æ€¥æ€§é˜‘å°¾ç‚ä¸­ï¼Œè¯ç‰©æ²»ç–—å¯ä»¥æ˜¯æœ‰æ•ˆçš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨æŠ—ç‚è¯ç‰©å¦‚é˜¿å¸åŒ¹æ—æ¥ç¼“è§£ç‚ç—‡ååº”ï¼ŒåŒæ—¶å¯ä»¥è¿›è¡Œæ— åˆ›æ²»ç–—å¦‚æ‰‹æœ¯æ¥å‡è½»ç—‡çŠ¶ã€‚æ ¹æ®ç—…å²å’Œååº”ï¼Œæ‚£è€…åº”ç«‹å³å¼€å§‹è¯ç‰©æ²»ç–—å’Œæ— åˆ›æ²»ç–—ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œæ›´æ·±å…¥çš„è¯„ä¼°å’Œæ²»ç–—ã€‚\n",
      "\n",
      "åœ¨å¤„ç†å‹ç—›åŒ…å—ä¹‹å‰ï¼Œç—…äººæœ€å¥½å°½å¿«è¿›è¡Œä¸€äº›è¯ç‰©æ²»ç–—ä»¥é˜²æ­¢æ„ŸæŸ“æ‰©æ•£å’Œé™ä½æ„ŸæŸ“é£é™©ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(output[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str, model, tokenizer, inference_prompt: str, max_new_tokens: int = 1024) -> str:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å’Œåˆ†è¯å™¨ä¸ºç»™å®šçš„åŒ»å­¦é—®é¢˜ç”Ÿæˆå“åº”ã€‚\n",
    "\n",
    "    Args:\n",
    "        question (str): éœ€è¦æ¨¡å‹å›ç­”çš„åŒ»å­¦é—®é¢˜ã€‚\n",
    "        model: å·²åŠ è½½çš„ Unsloth/Hugging Face æ¨¡å‹ã€‚\n",
    "        tokenizer: å¯¹åº”çš„åˆ†è¯å™¨ã€‚\n",
    "        inference_prompt (str): ç”¨äºæ ¼å¼åŒ–è¾“å…¥çš„ f-string æ¨¡æ¿ã€‚\n",
    "        max_new_tokens (int, optional): ç”Ÿæˆå“åº”çš„æœ€å¤§ token æ•°é‡ã€‚é»˜è®¤ä¸º 1024ã€‚\n",
    "\n",
    "    Returns:\n",
    "        str: æ¨¡å‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬ï¼Œå·²å»é™¤ prompt éƒ¨åˆ†ã€‚\n",
    "    \"\"\"\n",
    "    # 1. ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–è¾“å…¥\n",
    "    prompt = inference_prompt.format(\n",
    "        question, # å¡«å……é—®é¢˜\n",
    "        \"\",       # ç•™ç©ºï¼Œè®©æ¨¡å‹ç”Ÿæˆ CoT å’Œ Response\n",
    "    )\n",
    "\n",
    "    # 2. å°†æ ¼å¼åŒ–åçš„ prompt è¿›è¡Œåˆ†è¯ï¼Œå¹¶è½¬ç§»åˆ° GPU\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # 3. ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡º\n",
    "    # use_cache=True ç”¨äºåŠ é€Ÿè§£ç è¿‡ç¨‹\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    # 4. å°†ç”Ÿæˆçš„ token è§£ç ä¸ºæ–‡æœ¬\n",
    "    # skip_special_tokens=True ä¼šç§»é™¤åƒ EOS_TOKEN è¿™æ ·çš„ç‰¹æ®Šæ ‡è®°\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # 5. åˆ‡åˆ†å­—ç¬¦ä¸²ï¼Œåªè¿”å› \"### Response:\" ä¹‹åçš„éƒ¨åˆ†\n",
    "    # ä½¿ç”¨ .split() åˆ†å‰²å¹¶è·å–å“åº”å†…å®¹ï¼Œ.strip() ç”¨äºå»é™¤å¯èƒ½å­˜åœ¨çš„å‰åç©ºç™½å­—ç¬¦\n",
    "    response_part = decoded_output.split(\"### Response:\")\n",
    "    if len(response_part) > 1:\n",
    "        return response_part[1].strip()\n",
    "    else:\n",
    "        # å¦‚æœæ¨¡å‹æ²¡æœ‰ç”Ÿæˆ \"### Response:\" æ ‡è®°ï¼Œåˆ™è¿”å›æ•´ä¸ªç”Ÿæˆå†…å®¹ä»¥ä¾›è°ƒè¯•\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "å—¯ï¼Œè¿™ä½60å²ç”·æ€§æ‚£è€…ï¼Œæœ‰å³ä¾§èƒ¸ç–¼ï¼ŒXçº¿æ£€æŸ¥æ˜¾ç¤ºè‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¿™è®©æˆ‘æƒ³åˆ°å¯èƒ½æœ‰è‚ºç»“æ ¸çš„å¯èƒ½ã€‚è‚ºç»“æ ¸é€šå¸¸ä¼šå¸¦æ¥èƒ¸è…”ç§¯æ¶²ï¼Œä¹Ÿå°±æ˜¯è‚ºç»“æ ¸çš„å¸¸è§ç—‡çŠ¶ä¹‹ä¸€ã€‚\n",
      "\n",
      "é‚£ä¹ˆï¼Œæƒ³äº†è§£è¿™ä¸ªèƒ¸è…”ç§¯æ¶²çš„æ€§è´¨ï¼Œåº”è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿå—¯ï¼Œå®éªŒå®¤æ£€æŸ¥æ˜¯å…³é”®ã€‚å¬è¯´æœ‰å‡ ç§æ£€æŸ¥å¯ä»¥ç”¨æ¥åˆ¤æ–­èƒ¸è…”ç§¯æ¶²çš„æ€§è´¨ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œè¡€å¸¸è§„æ£€æŸ¥æ˜¯è‚¯å®šçš„ã€‚è¡€å¸¸è§„èƒ½å¸®åŠ©æˆ‘ä»¬äº†è§£è¡€ç»†èƒæ•°é‡ï¼Œå¦‚æœè¡€ç»†èƒå‡å°‘ï¼Œç‰¹åˆ«æ˜¯çº¢ç»†èƒï¼Œé‚£è¯´æ˜æœ‰è‚ºç»“æ ¸çš„å¯èƒ½ï¼Œå› ä¸ºè‚ºç»“æ ¸ä¼šå¯¼è‡´ä½è¡€å®¹é‡ã€‚\n",
      "\n",
      "å†æ¥çœ‹å°¿å¸¸è§„ï¼Œå¦‚æœå°¿æ¶²ä¸­å‡ºç°è¡€è‰²å’Œå°¿ç´ æ°®ï¼Œå°±è¯´æ˜æœ‰è¡€å°¿ï¼Œè¿™ä¹Ÿå’Œè‚ºç»“æ ¸çš„ç‰¹å¾ç›¸ç¬¦ã€‚\n",
      "\n",
      "è‡³äºè¡€æµ†åŸ¹å…»ï¼Œè¡€æµ†ä¸­çš„è›‹ç™½è´¨å«é‡æ˜¯å…³é”®ã€‚è‚ºç»“æ ¸ä¼šå¯¼è‡´è¡€æµ†ä¸­çš„è›‹ç™½è´¨å¢åŠ ï¼Œæ‰€ä»¥è¡€æµ†åŸ¹å…»ä¹Ÿå¾ˆé‡è¦ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ˜¯å¦æœ‰è¡€æµ†è›‹ç™½ã€‚\n",
      "\n",
      "è¡€æµ†åŸ¹å…»ç»“æœè¿˜èƒ½å¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ˜¯è‚ºç»“æ ¸è¿˜æ˜¯å…¶ä»–ç±»å‹çš„èƒ¸è…”ç§¯æ¶²ã€‚æ¯”å¦‚ï¼Œå¦‚æœè¡€æµ†è›‹ç™½å¢é«˜ï¼Œä½†æ²¡æœ‰è¡€å°¿ï¼Œé‚£å¯èƒ½æ˜¯è‚ºç»“æ ¸ï¼›è€Œå¦‚æœè¡€æµ†è›‹ç™½å¢é«˜ä¸”è¡€å°¿å‡ºç°ï¼Œé‚£å°±å¯èƒ½æ˜¯å…¶ä»–æƒ…å†µã€‚\n",
      "\n",
      "å—¯ï¼Œè¿™æ ·çœ‹æ¥ï¼Œè¡€æµ†åŸ¹å…»æ˜¯äº†è§£èƒ¸è…”ç§¯æ¶²æ€§è´¨çš„æœ€ç›´æ¥æ–¹æ³•ã€‚å› ä¸ºé€šè¿‡è¡€æµ†åŸ¹å…»ï¼Œæˆ‘ä»¬èƒ½æ›´å‡†ç¡®åœ°çŸ¥é“èƒ¸è…”ç§¯æ¶²çš„ç±»å‹å’Œæ€§è´¨ï¼Œè¿™å¯¹è¯Šæ–­å’Œæ²»ç–—éƒ½å¾ˆé‡è¦ã€‚\n",
      "</think>\n",
      "å¯¹äº60å²ç”·æ€§æ‚£è€…ï¼Œå‡ºç°å³ä¾§èƒ¸ç–¼ä¸”Xçº¿æ£€æŸ¥æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¿™æç¤ºæœ‰è‚ºç»“æ ¸çš„å¯èƒ½ã€‚ä¸ºäº†æ›´å¥½åœ°äº†è§£èƒ¸è…”ç§¯æ¶²çš„æ€§è´¨ï¼Œè¿›è¡Œè¡€æµ†åŸ¹å…»æ˜¯å…³é”®ã€‚è¡€æµ†åŸ¹å…»å¯ä»¥æä¾›è¡€æµ†è›‹ç™½æ°´å¹³ï¼Œå¸®åŠ©è¯Šæ–­è‚ºç»“æ ¸ï¼Œå› ä¸ºè‚ºç»“æ ¸ä¼šå¯¼è‡´è¡€æµ†è›‹ç™½å¢é«˜ã€‚å¦‚æœè¡€æµ†è›‹ç™½å¢é«˜ä¸”è¡€å°¿å‡ºç°ï¼Œå¯ä»¥åˆ¤æ–­ä¸ºè‚ºç»“æ ¸ï¼›å¦‚æœè¡€æµ†è›‹ç™½å¢é«˜ä½†æ²¡æœ‰è¡€å°¿ï¼Œå¯èƒ½æ˜¯å…¶ä»–ç±»å‹çš„èƒ¸è…”ç§¯æ¶²ã€‚å› æ­¤ï¼Œè¡€æµ†åŸ¹å…»æ˜¯äº†è§£èƒ¸è…”ç§¯æ¶²æ€§è´¨çš„æœ€ç›´æ¥æ–¹æ³•ã€‚\n"
     ]
    }
   ],
   "source": [
    "my_question = \"å¯¹äºä¸€å60å²ç”·æ€§æ‚£è€…ï¼Œå‡ºç°å³ä¾§èƒ¸ç–¼å¹¶åœ¨Xçº¿æ£€æŸ¥ä¸­æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¯Šæ–­ä¸ºè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ï¼Œè¯·é—®å“ªä¸€é¡¹å®éªŒå®¤æ£€æŸ¥å¯¹äº†è§£èƒ¸æ°´çš„æ€§è´¨æ›´æœ‰å¸®åŠ©ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model, tokenizer, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "è¿™ä½28å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œé•¿æœŸç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒï¼Œè¿™è®©æˆ‘æƒ³åˆ°ä»–å¯èƒ½å¾—äº†è„‘æŸä¼¤æ€§æ™•çœ©ç—‡ã€‚\n",
      "\n",
      "è„‘æŸä¼¤æ€§æ™•çœ©ç—‡æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»ç³»ç»Ÿç–¾ç—…ï¼Œå°¤å…¶æ˜¯è„‘æŸä¼¤åï¼Œå¯¼è‡´è„‘ç¼ºæ°§å’Œç¼ºè¡€ï¼Œä»è€Œå¼•èµ·æ™•çœ©ç­‰ç—‡çŠ¶ã€‚è¿™ç§ç–¾ç—…é€šå¸¸è¡¨ç°ä¸ºè„‘ç¼ºæ°§ï¼Œè¿›è€Œå¯¼è‡´æ™•çœ©ï¼Œå°¤å…¶åœ¨é•¿æ—¶é—´çš„è„‘æŸä¼¤åï¼Œæ™•çœ©ç—‡çŠ¶ä¼šæ›´åŠ æ˜æ˜¾ã€‚\n",
      "\n",
      "å†ä»”ç»†æƒ³æƒ³ï¼Œè¿™ä½æ‚£è€…é•¿æ—¶é—´ç†¬å¤œï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè„‘ç¼ºæ°§å¯¼è‡´çš„ç–²åŠ³ï¼Œç”šè‡³æ›´å¯èƒ½å¯¼è‡´è„‘æŸä¼¤æ€§æ™•çœ©ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ™•çœ©ä¸ä»…ä»…æ˜¯ç®€å•çš„ç–²åŠ³ï¼Œè€Œæ˜¯ä¸è„‘ç¼ºæ°§å’Œç¼ºè¡€ç›´æ¥ç›¸å…³ã€‚\n",
      "\n",
      "è„‘æŸä¼¤æ€§æ™•çœ©ç—‡çš„ç‰¹å¾åŒ…æ‹¬è„‘ç¼ºæ°§ã€ç¼ºè¡€ã€ç¼ºæ°§è¡€æµé‡å’Œè„‘ç¼ºæ°§è¡€æµé‡ï¼Œè¿™äº›éƒ½æ˜¯å¯¼è‡´æ™•çœ©çš„é‡è¦åŸå› ã€‚è¿™ç§ç–¾ç—…ä¹Ÿå¸¸ä¼´éšè„‘æ°´è‚¿å’Œè„‘æŸä¼¤ï¼Œæ‰€ä»¥æ‚£è€…çš„æƒ…å†µå¯èƒ½å’Œè„‘æ°´è‚¿æœ‰å…³ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œè„‘æŸä¼¤æ€§æ™•çœ©ç—‡çš„ç—…å› å…¶å®ä¸æ˜¯å•ä¸€çš„ï¼Œå®ƒä¸ä»…å—è„‘æŸä¼¤çš„å½±å“ï¼Œè¿˜å¯èƒ½å—åˆ°è„‘æŸä¼¤å‰è„‘æ°´è‚¿çš„å½±å“ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…å¯èƒ½éœ€è¦è€ƒè™‘è„‘æ°´è‚¿ä½œä¸ºå¯èƒ½çš„ç—…å› ã€‚\n",
      "\n",
      "æ€»çš„æ¥è¯´ï¼Œè¿™ä½æ‚£è€…å¯èƒ½å¾—äº†è„‘æŸä¼¤æ€§æ™•çœ©ç—‡ï¼Œå› ä¸ºä»–çš„ç—‡çŠ¶å’Œç—…å› éƒ½ç¬¦åˆè„‘æŸä¼¤åå‡ºç°çš„å…¸å‹è¡¨ç°ã€‚ä¸è¿‡ï¼Œç”±äºç—…å› å¤æ‚ï¼Œå»ºè®®ä»–å°½å¿«å»çœ‹è„‘æ°´è‚¿ä¸“å®¶ï¼Œä»¥æ’é™¤è„‘æ°´è‚¿çš„å¯èƒ½ï¼Œä»è€Œæ›´å¥½åœ°è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
      "</think>\n",
      "```\n",
      "æ ¹æ®æ‚£è€…çš„ç—‡çŠ¶å’Œç—…å› åˆ†æï¼Œè¿™ä½28å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œé•¿æœŸç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒï¼Œè¿™æç¤ºä»–å¯èƒ½å¾—äº†è„‘æŸä¼¤æ€§æ™•çœ©ç—‡ã€‚è„‘æŸä¼¤æ€§æ™•çœ©ç—‡æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»ç³»ç»Ÿç–¾ç—…ï¼Œå°¤å…¶æ˜¯è„‘æŸä¼¤åï¼Œå¯¼è‡´è„‘ç¼ºæ°§å’Œç¼ºè¡€ï¼Œä»è€Œå¼•èµ·æ™•çœ©ã€‚è¿™ç§ç–¾ç—…é€šå¸¸è¡¨ç°ä¸ºè„‘ç¼ºæ°§ï¼Œè¿›è€Œå¯¼è‡´æ™•çœ©ï¼Œå°¤å…¶åœ¨é•¿æ—¶é—´çš„è„‘æŸä¼¤åï¼Œæ™•çœ©ç—‡çŠ¶ä¼šæ›´åŠ æ˜æ˜¾ã€‚\n",
      "\n",
      "æ­¤å¤–ï¼Œè„‘æŸä¼¤æ€§æ™•çœ©ç—‡çš„ç—…å› ä¹Ÿå—è„‘æŸä¼¤å‰è„‘æ°´è‚¿çš„å½±å“ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…å¯èƒ½éœ€è¦è€ƒè™‘è„‘æ°´è‚¿ä½œä¸ºå¯èƒ½çš„ç—…å› ã€‚å› æ­¤ï¼Œå»ºè®®å°½å¿«æŸ¥çœ‹è„‘æ°´è‚¿ä¸“å®¶ï¼Œä»¥æ’é™¤è„‘æ°´è‚¿çš„å¯èƒ½ï¼Œä»è€Œæ›´å¥½åœ°è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
      "\n",
      "ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™ä½æ‚£è€…å¯èƒ½å¾—äº†è„‘æŸä¼¤æ€§æ™•çœ©ç—‡ï¼Œä½†å»ºè®®å°½å¿«å¯»æ±‚ä¸“ä¸šè¯Šæ–­ï¼Œä»¥ç¡®ä¿å…¶ç—…æƒ…å¾—åˆ°åŠæ—¶å‡†ç¡®çš„æ²»ç–—ã€‚\n"
     ]
    }
   ],
   "source": [
    "my_question = \"å¯¹äºä¸€å 28 å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œå¸¸å¹´ç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒã€‚è¯·é—®æœ‰å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model, tokenizer, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepseek)",
   "language": "python",
   "name": "deepseek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
